Epoch 1/100
Validation...
Validation loss: 1.1148, Accuracy: 0.3125
Epoch 1 completed.

Training...
Batch loss: 1.1313587427139282
Batch loss: 1.1275309324264526
Batch loss: 1.126652717590332
Batch loss: 1.0298713445663452
Batch loss: 1.0515064001083374
Batch loss: 1.1267662048339844
Batch loss: 1.1266858577728271
Batch loss: 1.1264820098876953
Batch loss: 1.147961974143982
Batch loss: 1.1052072048187256
Batch loss: 1.0568052530288696
Batch loss: 1.072922945022583
Batch loss: 1.0092231035232544
Epoch 2/100
Validation...
Validation loss: 1.1136, Accuracy: 0.3125
Epoch 2 completed.

Training...
Batch loss: 1.1254570484161377
Batch loss: 1.029325008392334
Batch loss: 1.0737240314483643
Batch loss: 1.0508872270584106
Batch loss: 1.077131986618042
Batch loss: 1.0764411687850952
Batch loss: 1.1526139974594116
Batch loss: 1.122718334197998
Batch loss: 1.1761245727539062
Batch loss: 1.0528244972229004
Batch loss: 1.125758409500122
Batch loss: 1.1255028247833252
Batch loss: 1.0962629318237305
Epoch 3/100
Validation...
Validation loss: 1.1126, Accuracy: 0.3125
Epoch 3 completed.

Training...
Batch loss: 1.1477265357971191
Batch loss: 1.1243517398834229
Batch loss: 1.0756289958953857
Batch loss: 1.0975011587142944
Batch loss: 1.0531655550003052
Batch loss: 1.0530458688735962
Batch loss: 1.0965949296951294
Batch loss: 1.1214673519134521
Batch loss: 1.0735228061676025
Batch loss: 1.0710117816925049
Batch loss: 1.1684675216674805
Batch loss: 1.0716378688812256
Batch loss: 1.0907173156738281
Epoch 4/100
Validation...
Validation loss: 1.1051, Accuracy: 0.3125
Epoch 4 completed.

Training...
Batch loss: 1.1370325088500977
Batch loss: 1.068300724029541
Batch loss: 1.090057134628296
Batch loss: 1.160323143005371
Batch loss: 1.065263032913208
Batch loss: 1.0816664695739746
Batch loss: 1.0328876972198486
Batch loss: 1.058183193206787
Batch loss: 1.0580590963363647
Batch loss: 1.0547198057174683
Batch loss: 1.0832421779632568
Batch loss: 1.073117733001709
Batch loss: 1.190299153327942
Epoch 5/100
Validation...
Validation loss: 1.0869, Accuracy: 0.3125
Epoch 5 completed.

Training...
Batch loss: 1.0193839073181152
Batch loss: 1.096828579902649
Batch loss: 1.1189285516738892
Batch loss: 1.0764976739883423
Batch loss: 1.093321442604065
Batch loss: 1.0237234830856323
Batch loss: 1.0906953811645508
Batch loss: 1.0579302310943604
Batch loss: 1.0343737602233887
Batch loss: 1.0015637874603271
Batch loss: 1.1074094772338867
Batch loss: 1.0510456562042236
Batch loss: 0.9499566555023193
Epoch 6/100
Validation...
Validation loss: 1.0662, Accuracy: 0.5625
Epoch 6 completed.

Training...
Batch loss: 0.9693645238876343
Batch loss: 1.0203959941864014
Batch loss: 1.07778000831604
Batch loss: 1.0735009908676147
Batch loss: 1.0746452808380127
Batch loss: 1.0307308435440063
Batch loss: 1.1293457746505737
Batch loss: 0.9873746633529663
Batch loss: 0.9616632461547852
Batch loss: 1.102437973022461
Batch loss: 0.9301890134811401
Batch loss: 1.0155506134033203
Batch loss: 1.1642570495605469
Epoch 7/100
Validation...
Validation loss: 1.0371, Accuracy: 0.5625
Epoch 7 completed.

Training...
Batch loss: 0.9876113533973694
Batch loss: 1.095094919204712
Batch loss: 1.0219600200653076
Batch loss: 1.0293397903442383
Batch loss: 1.0721596479415894
Batch loss: 1.0440285205841064
Batch loss: 0.9217600226402283
Batch loss: 0.9645142555236816
Batch loss: 0.9149537682533264
Batch loss: 0.9573221206665039
Batch loss: 0.9541274905204773
Batch loss: 1.0309195518493652
Batch loss: 0.9560559391975403
Epoch 8/100
Validation...
Validation loss: 1.0039, Accuracy: 0.5625
Epoch 8 completed.

Training...
Batch loss: 0.9930003881454468
Batch loss: 0.8578715920448303
Batch loss: 1.0999447107315063
Batch loss: 0.966048002243042
Batch loss: 0.9617735147476196
Batch loss: 0.9923892617225647
Batch loss: 0.9800238609313965
Batch loss: 0.9160380363464355
Batch loss: 0.8587582111358643
Batch loss: 0.9454938769340515
Batch loss: 0.8502130508422852
Batch loss: 1.0534108877182007
Batch loss: 1.147398829460144
Epoch 9/100
Validation...
Validation loss: 0.9670, Accuracy: 0.5625
Epoch 9 completed.

Training...
Batch loss: 0.9539238214492798
Batch loss: 0.9307706356048584
Batch loss: 0.9091243147850037
Batch loss: 0.8868498206138611
Batch loss: 0.9215997457504272
Batch loss: 0.8888521194458008
Batch loss: 0.915816605091095
Batch loss: 0.810541033744812
Batch loss: 1.1547883749008179
Batch loss: 0.872285008430481
Batch loss: 0.9392672181129456
Batch loss: 0.8694700002670288
Batch loss: 1.1756616830825806
Epoch 10/100
Validation...
Validation loss: 0.9532, Accuracy: 0.5625
Epoch 10 completed.

Training...
Batch loss: 1.0966659784317017
Batch loss: 0.821932852268219
Batch loss: 0.9700791239738464
Batch loss: 0.9735977649688721
Batch loss: 0.8509565591812134
Batch loss: 0.8379243612289429
Batch loss: 0.905388355255127
Batch loss: 0.7895654439926147
Batch loss: 1.066263198852539
Batch loss: 0.7724525928497314
Batch loss: 0.7930577993392944
Batch loss: 0.8863919973373413
Batch loss: 1.147046685218811
Epoch 11/100
Validation...
Validation loss: 0.9254, Accuracy: 0.5625
Epoch 11 completed.

Training...
Batch loss: 0.7863286137580872
Batch loss: 0.880824089050293
Batch loss: 1.088329553604126
Batch loss: 0.803065299987793
Batch loss: 0.8951079845428467
Batch loss: 0.940950870513916
Batch loss: 1.0103093385696411
Batch loss: 0.6951694488525391
Batch loss: 0.8144655227661133
Batch loss: 0.7434972524642944
Batch loss: 0.9174013733863831
Batch loss: 0.9327203035354614
Batch loss: 0.8420785069465637
Epoch 12/100
Validation...
Validation loss: 0.8985, Accuracy: 0.5625
Epoch 12 completed.

Training...
Batch loss: 1.054603099822998
Batch loss: 0.8572351336479187
Batch loss: 0.8540433645248413
Batch loss: 0.6743662357330322
Batch loss: 0.9246240854263306
Batch loss: 0.9764708876609802
Batch loss: 0.9233484864234924
Batch loss: 0.614937961101532
Batch loss: 0.7913707494735718
Batch loss: 0.7893270254135132
Batch loss: 0.8384063243865967
Batch loss: 0.9169979691505432
Batch loss: 0.80253005027771
Epoch 13/100
Validation...
Validation loss: 0.8861, Accuracy: 0.5625
Epoch 13 completed.

Training...
Batch loss: 0.9140344262123108
Batch loss: 0.8347526788711548
Batch loss: 0.9614770412445068
Batch loss: 0.781290590763092
Batch loss: 0.6910958290100098
Batch loss: 0.9569662809371948
Batch loss: 0.8232451677322388
Batch loss: 0.9209872484207153
Batch loss: 0.7711156010627747
Batch loss: 0.9043140411376953
Batch loss: 0.6360601186752319
Batch loss: 0.8591896295547485
Batch loss: 0.5882880091667175
Epoch 14/100
Validation...
Validation loss: 0.8699, Accuracy: 0.5625
Epoch 14 completed.

Training...
Batch loss: 0.9010959267616272
Batch loss: 0.7670456171035767
Batch loss: 0.8106859922409058
Batch loss: 0.8989970088005066
Batch loss: 0.768370509147644
Batch loss: 0.6238367557525635
Batch loss: 0.7463531494140625
Batch loss: 0.7983932495117188
Batch loss: 0.8945001363754272
Batch loss: 0.8964424133300781
Batch loss: 0.7544984221458435
Batch loss: 0.8533805012702942
Batch loss: 1.1296719312667847
Epoch 15/100
Validation...
Validation loss: 0.8586, Accuracy: 0.5625
Epoch 15 completed.

Training...
Batch loss: 0.7280429005622864
Batch loss: 0.8269916772842407
Batch loss: 1.0265696048736572
Batch loss: 0.887592077255249
Batch loss: 0.9238351583480835
Batch loss: 0.6830831170082092
Batch loss: 0.746941089630127
Batch loss: 0.8497073650360107
Batch loss: 0.7818502187728882
Batch loss: 0.710068941116333
Batch loss: 0.7083538770675659
Batch loss: 0.7774727940559387
Batch loss: 0.7059388756752014
Epoch 16/100
Validation...
Validation loss: 0.8455, Accuracy: 0.5625
Epoch 16 completed.

Training...
Batch loss: 0.636482834815979
Batch loss: 0.9117481112480164
Batch loss: 0.9108400344848633
Batch loss: 0.8768162131309509
Batch loss: 0.6305753588676453
Batch loss: 0.705653190612793
Batch loss: 0.70639967918396
Batch loss: 0.7658472061157227
Batch loss: 0.6250174641609192
Batch loss: 1.1213889122009277
Batch loss: 0.62299644947052
Batch loss: 0.8879703879356384
Batch loss: 1.120490312576294
Epoch 17/100
Validation...
Validation loss: 0.8415, Accuracy: 0.5625
Epoch 17 completed.

Training...
Batch loss: 0.6465362906455994
Batch loss: 0.8704806566238403
Batch loss: 0.8463015556335449
Batch loss: 0.754638671875
Batch loss: 0.5895264148712158
Batch loss: 0.8672646284103394
Batch loss: 0.7567793130874634
Batch loss: 0.8878747224807739
Batch loss: 0.889404296875
Batch loss: 0.7792953252792358
Batch loss: 0.7014901041984558
Batch loss: 0.7495369911193848
Batch loss: 1.1149673461914062
Epoch 18/100
Validation...
Validation loss: 0.8538, Accuracy: 0.5625
Epoch 18 completed.

Training...
Batch loss: 0.685835063457489
Batch loss: 0.7248042225837708
Batch loss: 0.6365199685096741
Batch loss: 0.8378354907035828
Batch loss: 0.8612170219421387
Batch loss: 0.8604299426078796
Batch loss: 0.7491172552108765
Batch loss: 0.9075343608856201
Batch loss: 1.1149448156356812
Batch loss: 0.7554113864898682
Batch loss: 0.6987781524658203
Batch loss: 0.7426261901855469
Batch loss: 0.560979425907135
Epoch 19/100
Validation...
Validation loss: 0.8290, Accuracy: 0.5625
Epoch 19 completed.

Training...
Batch loss: 0.7197398543357849
Batch loss: 0.88539057970047
Batch loss: 0.7415668368339539
Batch loss: 0.7189928293228149
Batch loss: 0.7194360494613647
Batch loss: 0.9923559427261353
Batch loss: 0.6992120742797852
Batch loss: 0.740989089012146
Batch loss: 0.8330958485603333
Batch loss: 0.6859813928604126
Batch loss: 0.6027504205703735
Batch loss: 0.8735507726669312
Batch loss: 1.1032425165176392
Epoch 20/100
Validation...
Validation loss: 0.8177, Accuracy: 0.5625
Epoch 20 completed.

Training...
Batch loss: 0.6217318177223206
Batch loss: 0.8302974104881287
Batch loss: 0.7148093581199646
Batch loss: 0.8497260808944702
Batch loss: 0.5985651016235352
Batch loss: 0.8493004441261292
Batch loss: 0.5973722338676453
Batch loss: 0.5968402028083801
Batch loss: 0.860451877117157
Batch loss: 0.9639936685562134
Batch loss: 0.8649542331695557
Batch loss: 0.729557991027832
Batch loss: 1.096882939338684
Epoch 21/100
Validation...
Validation loss: 0.8121, Accuracy: 0.5625
Epoch 21 completed.

Training...
Batch loss: 0.746460497379303
Batch loss: 0.7280759811401367
Batch loss: 0.9780502915382385
Batch loss: 0.7268133759498596
Batch loss: 0.70943683385849
Batch loss: 0.8423742055892944
Batch loss: 0.6997556090354919
Batch loss: 0.7078286409378052
Batch loss: 0.8405098915100098
Batch loss: 0.7237324714660645
Batch loss: 0.6905996799468994
Batch loss: 0.590186595916748
Batch loss: 1.089259147644043
Epoch 22/100
Validation...
Validation loss: 0.8052, Accuracy: 0.5625
Epoch 22 completed.

Training...
Batch loss: 0.5895801186561584
Batch loss: 0.5734360218048096
Batch loss: 0.5732622742652893
Batch loss: 0.9685949087142944
Batch loss: 0.7046486735343933
Batch loss: 0.8512120246887207
Batch loss: 0.8204677104949951
Batch loss: 0.7035265564918518
Batch loss: 0.849152684211731
Batch loss: 1.0819425582885742
Batch loss: 0.7188522815704346
Batch loss: 0.600496768951416
Batch loss: 0.5570828914642334
Epoch 23/100
Validation...
Validation loss: 0.7993, Accuracy: 0.5625
Epoch 23 completed.

Training...
Batch loss: 0.5712481737136841
Batch loss: 0.9618076086044312
Batch loss: 0.8309438824653625
Batch loss: 0.6865114569664001
Batch loss: 0.7138547301292419
Batch loss: 0.8431763052940369
Batch loss: 0.6995612978935242
Batch loss: 0.8416014313697815
Batch loss: 0.8282762765884399
Batch loss: 0.5829622149467468
Batch loss: 0.5954733490943909
Batch loss: 0.6852269172668457
Batch loss: 1.072267770767212
Epoch 24/100
Validation...
Validation loss: 0.7933, Accuracy: 0.5625
Epoch 24 completed.

Training...
Batch loss: 0.8128435015678406
Batch loss: 0.6967589855194092
Batch loss: 0.721179723739624
Batch loss: 0.8113546371459961
Batch loss: 0.8110479116439819
Batch loss: 0.8105724453926086
Batch loss: 0.6948478817939758
Batch loss: 0.8210827112197876
Batch loss: 0.5919088125228882
Batch loss: 0.7182658910751343
Batch loss: 0.579490065574646
Batch loss: 0.7048529386520386
Batch loss: 1.0587443113327026
Epoch 25/100
Validation...
Validation loss: 0.7865, Accuracy: 0.5625
Epoch 25 completed.

Training...
Batch loss: 0.9316478967666626
Batch loss: 0.8177263736724854
Batch loss: 0.567089319229126
Batch loss: 0.5668647885322571
Batch loss: 0.7021773457527161
Batch loss: 0.7019001245498657
Batch loss: 0.7037920951843262
Batch loss: 0.6793570518493652
Batch loss: 0.9362131953239441
Batch loss: 0.6999855637550354
Batch loss: 0.8120062351226807
Batch loss: 0.7067122459411621
Batch loss: 0.5968962907791138
Epoch 26/100
Validation...
Validation loss: 0.7799, Accuracy: 0.5625
Epoch 26 completed.

Training...
Batch loss: 0.5657597780227661
Batch loss: 0.5757749676704407
Batch loss: 0.8100962042808533
Batch loss: 0.8188710808753967
Batch loss: 0.8087366819381714
Batch loss: 0.930975079536438
Batch loss: 0.6861282587051392
Batch loss: 0.6861896514892578
Batch loss: 0.5837801694869995
Batch loss: 0.6860170960426331
Batch loss: 0.8060216903686523
Batch loss: 0.8046598434448242
Batch loss: 0.5553839802742004
Epoch 27/100
Validation...
Validation loss: 0.7738, Accuracy: 0.5625
Epoch 27 completed.

Training...
Batch loss: 0.812549352645874
Batch loss: 0.6834143996238708
Batch loss: 0.692992627620697
Batch loss: 0.7006362676620483
Batch loss: 0.5725265741348267
Batch loss: 0.6912432909011841
Batch loss: 0.7916706800460815
Batch loss: 0.5634685754776001
Batch loss: 0.7916206121444702
Batch loss: 0.790642261505127
Batch loss: 0.7952380776405334
Batch loss: 0.6966007947921753
Batch loss: 1.0230365991592407
Epoch 28/100
Validation...
Validation loss: 0.7670, Accuracy: 0.5625
Epoch 28 completed.

Training...
Batch loss: 0.8040967583656311
Batch loss: 0.6796725988388062
Batch loss: 0.6790167689323425
Batch loss: 0.686760663986206
Batch loss: 0.5709216594696045
Batch loss: 0.7934695482254028
Batch loss: 0.5707331895828247
Batch loss: 0.5706205368041992
Batch loss: 0.8978834748268127
Batch loss: 0.8949812650680542
Batch loss: 0.6683166027069092
Batch loss: 0.6911835670471191
Batch loss: 1.0070525407791138
Epoch 29/100
Validation...
Validation loss: 0.7590, Accuracy: 0.5625
Epoch 29 completed.

Training...
Batch loss: 0.7874157428741455
Batch loss: 0.7874822020530701
Batch loss: 0.7864291667938232
Batch loss: 0.5780266523361206
Batch loss: 0.5702257752418518
Batch loss: 0.7833442687988281
Batch loss: 0.6638913154602051
Batch loss: 0.7809875011444092
Batch loss: 0.5698838233947754
Batch loss: 0.6708477139472961
Batch loss: 0.7868159413337708
Batch loss: 0.6691948175430298
Batch loss: 0.9862834811210632
Epoch 30/100
Validation...
Validation loss: 0.7492, Accuracy: 1.0000
Epoch 30 completed.

Training...
Batch loss: 0.669314444065094
Batch loss: 0.6837903261184692
Batch loss: 0.7730379104614258
Batch loss: 0.6603448390960693
Batch loss: 0.7664510011672974
Batch loss: 0.7732257843017578
Batch loss: 0.6734709143638611
Batch loss: 0.5770982503890991
Batch loss: 0.6581491827964783
Batch loss: 0.7757774591445923
Batch loss: 0.7599799633026123
Batch loss: 0.5767840147018433
Batch loss: 0.9601913094520569
Epoch 31/100
Validation...
Validation loss: 0.7401, Accuracy: 1.0000
Epoch 31 completed.

Training...
Batch loss: 0.6629117727279663
Batch loss: 0.7633090615272522
Batch loss: 0.6611275672912598
Batch loss: 0.5615968704223633
Batch loss: 0.6744372844696045
Batch loss: 0.8537194728851318
Batch loss: 0.7645261883735657
Batch loss: 0.7595487833023071
Batch loss: 0.5691533088684082
Batch loss: 0.850113570690155
Batch loss: 0.6564037799835205
Batch loss: 0.584502637386322
Batch loss: 0.5539703369140625
Epoch 32/100
Validation...
Validation loss: 0.7268, Accuracy: 1.0000
Epoch 32 completed.

Training...
Batch loss: 0.7425104379653931
Batch loss: 0.7420706748962402
Batch loss: 0.8428432941436768
Batch loss: 0.5844392776489258
Batch loss: 0.6689097285270691
Batch loss: 0.5614323616027832
Batch loss: 0.6599102020263672
Batch loss: 0.5687980055809021
Batch loss: 0.6529184579849243
Batch loss: 0.7452648878097534
Batch loss: 0.7322753667831421
Batch loss: 0.73818439245224
Batch loss: 0.553827702999115
Epoch 33/100
Validation...
Validation loss: 0.7182, Accuracy: 1.0000
Epoch 33 completed.

Training...
Batch loss: 0.6566545963287354
Batch loss: 0.5609389543533325
Batch loss: 0.6545778512954712
Batch loss: 0.6615357398986816
Batch loss: 0.6428107619285583
Batch loss: 0.7447938919067383
Batch loss: 0.8117483854293823
Batch loss: 0.6528876423835754
Batch loss: 0.5606721639633179
Batch loss: 0.8148860335350037
Batch loss: 0.6449970006942749
Batch loss: 0.729255199432373
Batch loss: 0.5536239743232727
Epoch 34/100
Validation...
Validation loss: 0.7061, Accuracy: 1.0000
Epoch 34 completed.

Training...
Batch loss: 0.6521046161651611
Batch loss: 0.5677850842475891
Batch loss: 0.7253177165985107
Batch loss: 0.5606786012649536
Batch loss: 0.6432642936706543
Batch loss: 0.7162735462188721
Batch loss: 0.7156487703323364
Batch loss: 0.7282028198242188
Batch loss: 0.6475697755813599
Batch loss: 0.6468137502670288
Batch loss: 0.7180063128471375
Batch loss: 0.6453325152397156
Batch loss: 0.8617373704910278
Epoch 35/100
Validation...
Validation loss: 0.6937, Accuracy: 1.0000
Epoch 35 completed.

Training...
Batch loss: 0.6444622874259949
Batch loss: 0.7180846929550171
Batch loss: 0.7097770571708679
Batch loss: 0.6337267756462097
Batch loss: 0.7778828144073486
Batch loss: 0.7047836780548096
Batch loss: 0.6382100582122803
Batch loss: 0.6231598854064941
Batch loss: 0.695928692817688
Batch loss: 0.6220607161521912
Batch loss: 0.567378044128418
Batch loss: 0.5744729042053223
Batch loss: 0.5814234614372253
Epoch 36/100
Validation...
Validation loss: 0.6804, Accuracy: 1.0000
Epoch 36 completed.

Training...
Batch loss: 0.6331995725631714
Batch loss: 0.5602466464042664
Batch loss: 0.6921831369400024
Batch loss: 0.6383498311042786
Batch loss: 0.5532875061035156
Batch loss: 0.7475666999816895
Batch loss: 0.5733805894851685
Batch loss: 0.6302341222763062
Batch loss: 0.6321765184402466
Batch loss: 0.6866245865821838
Batch loss: 0.6796352863311768
Batch loss: 0.6914269328117371
Batch loss: 0.8002308011054993
Epoch 37/100
Validation...
Validation loss: 0.6669, Accuracy: 1.0000
Epoch 37 completed.

Training...
Batch loss: 0.7442947626113892
Batch loss: 0.620160698890686
Batch loss: 0.6195459961891174
Batch loss: 0.6249674558639526
Batch loss: 0.617721438407898
Batch loss: 0.5726478099822998
Batch loss: 0.6837307214736938
Batch loss: 0.622077465057373
Batch loss: 0.5531924962997437
Batch loss: 0.7194069623947144
Batch loss: 0.6707214713096619
Batch loss: 0.6209502220153809
Batch loss: 0.5791544914245605
Epoch 38/100
Validation...
Validation loss: 0.6531, Accuracy: 1.0000
Epoch 38 completed.

Training...
Batch loss: 0.6656088829040527
Batch loss: 0.6124668717384338
Batch loss: 0.6126209497451782
Batch loss: 0.6179528832435608
Batch loss: 0.5594372749328613
Batch loss: 0.6615043878555298
Batch loss: 0.5718244910240173
Batch loss: 0.6660789847373962
Batch loss: 0.6090017557144165
Batch loss: 0.6081308126449585
Batch loss: 0.7049671411514282
Batch loss: 0.6551938652992249
Batch loss: 0.5530802607536316
Epoch 39/100
Validation...
Validation loss: 0.6436, Accuracy: 1.0000
Epoch 39 completed.

Training...
Batch loss: 0.605764627456665
Batch loss: 0.5999065041542053
Batch loss: 0.6502636671066284
Batch loss: 0.6432436108589172
Batch loss: 0.5705181956291199
Batch loss: 0.5980989336967468
Batch loss: 0.6542118787765503
Batch loss: 0.6870059967041016
Batch loss: 0.5646853446960449
Batch loss: 0.6135969161987305
Batch loss: 0.5763217210769653
Batch loss: 0.6371632218360901
Batch loss: 0.7190439105033875
Epoch 40/100
Validation...
Validation loss: 0.6320, Accuracy: 1.0000
Epoch 40 completed.

Training...
Batch loss: 0.6410659551620483
Batch loss: 0.5992556214332581
Batch loss: 0.5993381142616272
Batch loss: 0.5701886415481567
Batch loss: 0.6312796473503113
Batch loss: 0.6301710605621338
Batch loss: 0.6413185596466064
Batch loss: 0.6339848637580872
Batch loss: 0.6328495740890503
Batch loss: 0.5955572724342346
Batch loss: 0.5700329542160034
Batch loss: 0.5641200542449951
Batch loss: 0.6981770396232605
Epoch 41/100
Validation...
Validation loss: 0.6219, Accuracy: 1.0000
Epoch 41 completed.

Training...
Batch loss: 0.5584331154823303
Batch loss: 0.5695774555206299
Batch loss: 0.6213163733482361
Batch loss: 0.5978291630744934
Batch loss: 0.626043975353241
Batch loss: 0.6251118183135986
Batch loss: 0.5912458896636963
Batch loss: 0.6233879327774048
Batch loss: 0.5906223654747009
Batch loss: 0.6006883382797241
Batch loss: 0.6216550469398499
Batch loss: 0.6204202175140381
Batch loss: 0.5528871417045593
Epoch 42/100
Validation...
Validation loss: 0.6133, Accuracy: 1.0000
Epoch 42 completed.

Training...
Batch loss: 0.5580905675888062
Batch loss: 0.6185814142227173
Batch loss: 0.6177732944488525
Batch loss: 0.6114578247070312
Batch loss: 0.5870262384414673
Batch loss: 0.5919806361198425
Batch loss: 0.6433339715003967
Batch loss: 0.5965732336044312
Batch loss: 0.5906857252120972
Batch loss: 0.5851907730102539
Batch loss: 0.587014377117157
Batch loss: 0.585353434085846
Batch loss: 0.5728117823600769
Epoch 43/100
Validation...
Validation loss: 0.6058, Accuracy: 1.0000
Epoch 43 completed.

Training...
Batch loss: 0.5940468907356262
Batch loss: 0.5840038061141968
Batch loss: 0.5832867622375488
Batch loss: 0.6035988330841064
Batch loss: 0.6125971078872681
Batch loss: 0.5823982954025269
Batch loss: 0.582617998123169
Batch loss: 0.5667871236801147
Batch loss: 0.5816548466682434
Batch loss: 0.5858679413795471
Batch loss: 0.6044908761978149
Batch loss: 0.622407853603363
Batch loss: 0.5528088212013245
Epoch 44/100
Validation...
Validation loss: 0.5972, Accuracy: 1.0000
Epoch 44 completed.

Training...
Batch loss: 0.588280975818634
Batch loss: 0.6056026220321655
Batch loss: 0.6174090504646301
Batch loss: 0.6159152984619141
Batch loss: 0.5980600118637085
Batch loss: 0.5570491552352905
Batch loss: 0.5528404712677002
Batch loss: 0.5800167322158813
Batch loss: 0.5936621427536011
Batch loss: 0.5610868334770203
Batch loss: 0.5784371495246887
Batch loss: 0.573898196220398
Batch loss: 0.5690430998802185
Epoch 45/100
Validation...
Validation loss: 0.5871, Accuracy: 1.0000
Epoch 45 completed.

Training...
Batch loss: 0.5906565189361572
Batch loss: 0.5725951790809631
Batch loss: 0.5894785523414612
Batch loss: 0.5606379508972168
Batch loss: 0.5566619634628296
Batch loss: 0.5643954277038574
Batch loss: 0.5956180691719055
Batch loss: 0.5960195064544678
Batch loss: 0.5843318104743958
Batch loss: 0.5734825134277344
Batch loss: 0.5731097459793091
Batch loss: 0.568963348865509
Batch loss: 0.5677090883255005
Epoch 46/100
Validation...
Validation loss: 0.5780, Accuracy: 1.0000
Epoch 46 completed.

Training...
Batch loss: 0.5527400970458984
Batch loss: 0.5836326479911804
Batch loss: 0.5756071209907532
Batch loss: 0.5674847960472107
Batch loss: 0.5707926750183105
Batch loss: 0.5669496059417725
Batch loss: 0.563213586807251
Batch loss: 0.5799150466918945
Batch loss: 0.5660589337348938
Batch loss: 0.5914648771286011
Batch loss: 0.5593475699424744
Batch loss: 0.5684926509857178
Batch loss: 0.5881176590919495
Epoch 47/100
Validation...
Validation loss: 0.5716, Accuracy: 1.0000
Epoch 47 completed.

Training...
Batch loss: 0.5764855146408081
Batch loss: 0.5726574063301086
Batch loss: 0.5671332478523254
Batch loss: 0.5667560696601868
Batch loss: 0.5602415800094604
Batch loss: 0.5677220821380615
Batch loss: 0.5628260374069214
Batch loss: 0.5665723085403442
Batch loss: 0.5587974786758423
Batch loss: 0.5617986917495728
Batch loss: 0.5684252977371216
Batch loss: 0.5649533867835999
Batch loss: 0.5764206647872925
Epoch 48/100
Validation...
Validation loss: 0.5664, Accuracy: 1.0000
Epoch 48 completed.

Training...
Batch loss: 0.5587456226348877
Batch loss: 0.5697379112243652
Batch loss: 0.561109185218811
Batch loss: 0.5696508884429932
Batch loss: 0.5611604452133179
Batch loss: 0.5681880712509155
Batch loss: 0.560424268245697
Batch loss: 0.5602821111679077
Batch loss: 0.5667581558227539
Batch loss: 0.5580342411994934
Batch loss: 0.5598477721214294
Batch loss: 0.5622907280921936
Batch loss: 0.5634214878082275
Epoch 49/100
Validation...
Validation loss: 0.5630, Accuracy: 1.0000
Epoch 49 completed.

Training...
Batch loss: 0.5578741431236267
Batch loss: 0.5608363747596741
Batch loss: 0.5617589950561523
Batch loss: 0.555126965045929
Batch loss: 0.5589081645011902
Batch loss: 0.5628135204315186
Batch loss: 0.5683863162994385
Batch loss: 0.5625382661819458
Batch loss: 0.5623102188110352
Batch loss: 0.5594791173934937
Batch loss: 0.5628876686096191
Batch loss: 0.5581740736961365
Batch loss: 0.5525951981544495
Epoch 50/100
Validation...
Validation loss: 0.5610, Accuracy: 1.0000
Epoch 50 completed.

Training...
Batch loss: 0.560291588306427
Batch loss: 0.5579831600189209
Batch loss: 0.5579769015312195
Batch loss: 0.5590072870254517
Batch loss: 0.5607689619064331
Batch loss: 0.5588866472244263
Batch loss: 0.5637505054473877
Batch loss: 0.5545303821563721
Batch loss: 0.5615348815917969
Batch loss: 0.5592646598815918
Batch loss: 0.5604076385498047
Batch loss: 0.5582114458084106
Batch loss: 0.552623987197876
Epoch 51/100
Validation...
Validation loss: 0.5595, Accuracy: 1.0000
Epoch 51 completed.

Training...
Batch loss: 0.5634602308273315
Batch loss: 0.5578716993331909
Batch loss: 0.5606335401535034
Batch loss: 0.5597540140151978
Batch loss: 0.5568703413009644
Batch loss: 0.559590220451355
Batch loss: 0.5584030151367188
Batch loss: 0.5559493899345398
Batch loss: 0.5590734481811523
Batch loss: 0.5548945069313049
Batch loss: 0.5605895519256592
Batch loss: 0.5525510907173157
Batch loss: 0.5524695515632629
Epoch 52/100
Validation...
Validation loss: 0.5583, Accuracy: 1.0000
Epoch 52 completed.

Training...
Batch loss: 0.5586016774177551
Batch loss: 0.5572413206100464
Batch loss: 0.5546357035636902
Batch loss: 0.5605382919311523
Batch loss: 0.556086540222168
Batch loss: 0.5576785802841187
Batch loss: 0.5554623603820801
Batch loss: 0.5545704364776611
Batch loss: 0.5583180785179138
Batch loss: 0.5561079978942871
Batch loss: 0.5566174983978271
Batch loss: 0.5606231689453125
Batch loss: 0.5580401420593262
Epoch 53/100
Validation...
Validation loss: 0.5575, Accuracy: 1.0000
Epoch 53 completed.

Training...
Batch loss: 0.5562981367111206
Batch loss: 0.5597727298736572
Batch loss: 0.557854175567627
Batch loss: 0.5556569695472717
Batch loss: 0.55756676197052
Batch loss: 0.5550270080566406
Batch loss: 0.5555905103683472
Batch loss: 0.5573943257331848
Batch loss: 0.5566648244857788
Batch loss: 0.5590049028396606
Batch loss: 0.553656816482544
Batch loss: 0.5536275506019592
Batch loss: 0.5592268705368042
Epoch 54/100
Validation...
Validation loss: 0.5568, Accuracy: 1.0000
Epoch 54 completed.

Training...
Batch loss: 0.5553227066993713
Batch loss: 0.5548475384712219
Batch loss: 0.5582245588302612
Batch loss: 0.5551403164863586
Batch loss: 0.5563778877258301
Batch loss: 0.5547246932983398
Batch loss: 0.5539449453353882
Batch loss: 0.55645352602005
Batch loss: 0.5546493530273438
Batch loss: 0.5584372878074646
Batch loss: 0.556156575679779
Batch loss: 0.556444525718689
Batch loss: 0.5580693483352661
Epoch 55/100
Validation...
Validation loss: 0.5562, Accuracy: 1.0000
Epoch 55 completed.

Training...
Batch loss: 0.5552449226379395
Batch loss: 0.5560225248336792
Batch loss: 0.5575453639030457
Batch loss: 0.5547381043434143
Batch loss: 0.5570252537727356
Batch loss: 0.555945873260498
Batch loss: 0.5547241568565369
Batch loss: 0.5547100305557251
Batch loss: 0.5555375814437866
Batch loss: 0.5534483194351196
Batch loss: 0.5547678470611572
Batch loss: 0.5570145845413208
Batch loss: 0.5522821545600891
Epoch 56/100
Validation...
Validation loss: 0.5556, Accuracy: 1.0000
Epoch 56 completed.

Training...
Batch loss: 0.5557945370674133
Batch loss: 0.5533940196037292
Batch loss: 0.554478108882904
Batch loss: 0.5556753277778625
Batch loss: 0.5556842088699341
Batch loss: 0.5544251203536987
Batch loss: 0.5564440488815308
Batch loss: 0.5533499717712402
Batch loss: 0.5553333759307861
Batch loss: 0.5563498735427856
Batch loss: 0.555378794670105
Batch loss: 0.5543384552001953
Batch loss: 0.5565605759620667
Epoch 57/100
Validation...
Validation loss: 0.5552, Accuracy: 1.0000
Epoch 57 completed.

Training...
Batch loss: 0.554314374923706
Batch loss: 0.5562016367912292
Batch loss: 0.5532391667366028
Batch loss: 0.5552854537963867
Batch loss: 0.5543171167373657
Batch loss: 0.5551181435585022
Batch loss: 0.555111289024353
Batch loss: 0.5522199869155884
Batch loss: 0.5561336278915405
Batch loss: 0.5549882650375366
Batch loss: 0.5559418201446533
Batch loss: 0.5540159940719604
Batch loss: 0.5558956861495972
Epoch 58/100
Validation...
Validation loss: 0.5548, Accuracy: 1.0000
Epoch 58 completed.

Training...
Batch loss: 0.5558531880378723
Batch loss: 0.555772066116333
Batch loss: 0.5539858937263489
Batch loss: 0.5540212392807007
Batch loss: 0.5556265115737915
Batch loss: 0.5548589825630188
Batch loss: 0.5539274215698242
Batch loss: 0.5539366602897644
Batch loss: 0.5539198517799377
Batch loss: 0.5547690391540527
Batch loss: 0.5547084808349609
Batch loss: 0.5530761480331421
Batch loss: 0.5522878170013428
Epoch 59/100
Validation...
Validation loss: 0.5545, Accuracy: 1.0000
Epoch 59 completed.

Training...
Batch loss: 0.5546984076499939
Batch loss: 0.5546051263809204
Batch loss: 0.5538129210472107
Batch loss: 0.555436909198761
Batch loss: 0.5545940399169922
Batch loss: 0.5537909865379333
Batch loss: 0.5545770525932312
Batch loss: 0.5552763938903809
Batch loss: 0.5528885126113892
Batch loss: 0.5528963804244995
Batch loss: 0.5552842617034912
Batch loss: 0.5536271929740906
Batch loss: 0.5522595643997192
Epoch 60/100
Validation...
Validation loss: 0.5543, Accuracy: 1.0000
Epoch 60 completed.

Training...
Batch loss: 0.553688645362854
Batch loss: 0.5543027520179749
Batch loss: 0.5543179512023926
Batch loss: 0.5529159903526306
Batch loss: 0.5543619394302368
Batch loss: 0.554348349571228
Batch loss: 0.5536489486694336
Batch loss: 0.5542718768119812
Batch loss: 0.5549683570861816
Batch loss: 0.5536354184150696
Batch loss: 0.5549806356430054
Batch loss: 0.5528721809387207
Batch loss: 0.5547538995742798
Epoch 61/100
Validation...
Validation loss: 0.5541, Accuracy: 1.0000
Epoch 61 completed.

Training...
Batch loss: 0.5535203218460083
Batch loss: 0.5542506575584412
Batch loss: 0.5541496872901917
Batch loss: 0.5542283654212952
Batch loss: 0.5535805225372314
Batch loss: 0.55338454246521
Batch loss: 0.5528131127357483
Batch loss: 0.5540813207626343
Batch loss: 0.5541139841079712
Batch loss: 0.5546250939369202
Batch loss: 0.5533611178398132
Batch loss: 0.5546908974647522
Batch loss: 0.5521660447120667
Epoch 62/100
Validation...
Validation loss: 0.5539, Accuracy: 1.0000
Epoch 62 completed.

Training...
Batch loss: 0.5547886490821838
Batch loss: 0.5528073906898499
Batch loss: 0.5540807247161865
Batch loss: 0.5540088415145874
Batch loss: 0.5527846217155457
Batch loss: 0.5526342391967773
Batch loss: 0.5539722442626953
Batch loss: 0.5545108318328857
Batch loss: 0.5539400577545166
Batch loss: 0.5538158416748047
Batch loss: 0.5533209443092346
Batch loss: 0.5537135601043701
Batch loss: 0.554180383682251
Epoch 63/100
Validation...
Validation loss: 0.5537, Accuracy: 1.0000
Epoch 63 completed.

Training...
Batch loss: 0.5537917613983154
Batch loss: 0.5537833571434021
Batch loss: 0.5539276599884033
Batch loss: 0.5540245771408081
Batch loss: 0.5532194375991821
Batch loss: 0.5537542104721069
Batch loss: 0.5525519251823425
Batch loss: 0.5540032982826233
Batch loss: 0.5536681413650513
Batch loss: 0.5530802011489868
Batch loss: 0.5531286001205444
Batch loss: 0.553732693195343
Batch loss: 0.5541200637817383
Epoch 64/100
Validation...
Validation loss: 0.5536, Accuracy: 1.0000
Epoch 64 completed.

Training...
Batch loss: 0.5535792708396912
Batch loss: 0.5539342761039734
Batch loss: 0.5532978177070618
Batch loss: 0.5539193749427795
Batch loss: 0.5535664558410645
Batch loss: 0.5529977083206177
Batch loss: 0.5533654689788818
Batch loss: 0.5537124872207642
Batch loss: 0.5530959963798523
Batch loss: 0.5524439811706543
Batch loss: 0.5534054040908813
Batch loss: 0.5542395710945129
Batch loss: 0.5519424676895142
Epoch 65/100
Validation...
Validation loss: 0.5534, Accuracy: 1.0000
Epoch 65 completed.

Training...
Batch loss: 0.5534577369689941
Batch loss: 0.5534600019454956
Batch loss: 0.5540549755096436
Batch loss: 0.55303955078125
Batch loss: 0.5530442595481873
Batch loss: 0.5532357096672058
Batch loss: 0.5534632205963135
Batch loss: 0.5535694360733032
Batch loss: 0.5525978207588196
Batch loss: 0.5529512166976929
Batch loss: 0.5532844662666321
Batch loss: 0.5535779595375061
Batch loss: 0.5536060929298401
Epoch 66/100
Validation...
Validation loss: 0.5533, Accuracy: 1.0000
Epoch 66 completed.

Training...
Batch loss: 0.5523841977119446
Batch loss: 0.5524327158927917
Batch loss: 0.553733766078949
Batch loss: 0.5535337328910828
Batch loss: 0.5540823936462402
Batch loss: 0.5531270503997803
Batch loss: 0.5533075332641602
Batch loss: 0.5539928078651428
Batch loss: 0.5529188513755798
Batch loss: 0.5534250140190125
Batch loss: 0.5532835125923157
Batch loss: 0.5526368618011475
Batch loss: 0.5520341396331787
Epoch 67/100
Validation...
Validation loss: 0.5532, Accuracy: 1.0000
Epoch 67 completed.

Training...
Batch loss: 0.5539883375167847
Batch loss: 0.5534290671348572
Batch loss: 0.5531429052352905
Batch loss: 0.5528774261474609
Batch loss: 0.5530544519424438
Batch loss: 0.5525017976760864
Batch loss: 0.5528562068939209
Batch loss: 0.5528786778450012
Batch loss: 0.552614688873291
Batch loss: 0.5532187819480896
Batch loss: 0.5530080199241638
Batch loss: 0.5536363124847412
Batch loss: 0.5540421605110168
Epoch 68/100
Validation...
Validation loss: 0.5531, Accuracy: 1.0000
Epoch 68 completed.

Training...
Batch loss: 0.5531318187713623
Batch loss: 0.5533111691474915
Batch loss: 0.5531330704689026
Batch loss: 0.5536342263221741
Batch loss: 0.553105890750885
Batch loss: 0.5529941320419312
Batch loss: 0.5529952645301819
Batch loss: 0.5536139607429504
Batch loss: 0.5532750487327576
Batch loss: 0.5528163909912109
Batch loss: 0.5524581074714661
Batch loss: 0.5522258877754211
Batch loss: 0.5520132184028625
Epoch 69/100
Validation...
Validation loss: 0.5530, Accuracy: 1.0000
Epoch 69 completed.

Training...
Batch loss: 0.5524274110794067
Batch loss: 0.5527567863464355
Batch loss: 0.5530374050140381
Batch loss: 0.5532337427139282
Batch loss: 0.5530610084533691
Batch loss: 0.5528411269187927
Batch loss: 0.5534660220146179
Batch loss: 0.5531846284866333
Batch loss: 0.5526400804519653
Batch loss: 0.5531623959541321
Batch loss: 0.5530259609222412
Batch loss: 0.5528850555419922
Batch loss: 0.5518637895584106
Epoch 70/100
Validation...
Validation loss: 0.5529, Accuracy: 1.0000
Epoch 70 completed.

Training...
Batch loss: 0.5524885058403015
Batch loss: 0.5524986982345581
Batch loss: 0.5527057647705078
Batch loss: 0.5523797273635864
Batch loss: 0.5531195402145386
Batch loss: 0.5530322790145874
Batch loss: 0.5528430938720703
Batch loss: 0.5531119108200073
Batch loss: 0.5532909035682678
Batch loss: 0.5533939599990845
Batch loss: 0.5526230335235596
Batch loss: 0.553093671798706
Batch loss: 0.5529844760894775
Epoch 71/100
Validation...
Validation loss: 0.5529, Accuracy: 1.0000
Epoch 71 completed.

Training...
Batch loss: 0.5533537864685059
Batch loss: 0.5523333549499512
Batch loss: 0.5532687306404114
Batch loss: 0.5524090528488159
Batch loss: 0.5524243116378784
Batch loss: 0.55286705493927
Batch loss: 0.5530738234519958
Batch loss: 0.5521949529647827
Batch loss: 0.5532913208007812
Batch loss: 0.5525873899459839
Batch loss: 0.5526093244552612
Batch loss: 0.5532016754150391
Batch loss: 0.5536377429962158
Epoch 72/100
Validation...
Validation loss: 0.5528, Accuracy: 1.0000
Epoch 72 completed.

Training...
Batch loss: 0.5528208613395691
Batch loss: 0.5532087683677673
Batch loss: 0.5521276593208313
Batch loss: 0.5525376200675964
Batch loss: 0.5523076057434082
Batch loss: 0.5531840324401855
Batch loss: 0.5529617667198181
Batch loss: 0.5521271228790283
Batch loss: 0.5529771447181702
Batch loss: 0.5529817938804626
Batch loss: 0.5527939796447754
Batch loss: 0.5529870986938477
Batch loss: 0.5529037117958069
Epoch 73/100
Validation...
Validation loss: 0.5527, Accuracy: 1.0000
Epoch 73 completed.

Training...
Batch loss: 0.5527003407478333
Batch loss: 0.5525286793708801
Batch loss: 0.552924394607544
Batch loss: 0.5527274012565613
Batch loss: 0.5527163147926331
Batch loss: 0.552280604839325
Batch loss: 0.5527416467666626
Batch loss: 0.5531216263771057
Batch loss: 0.5529177784919739
Batch loss: 0.5522639751434326
Batch loss: 0.5529006719589233
Batch loss: 0.5523375868797302
Batch loss: 0.5534594655036926
Epoch 74/100
Validation...
Validation loss: 0.5527, Accuracy: 1.0000
Epoch 74 completed.

Training...
Batch loss: 0.5526577234268188
Batch loss: 0.5524958372116089
Batch loss: 0.5530362129211426
Batch loss: 0.5522798299789429
Batch loss: 0.5528528690338135
Batch loss: 0.5528119802474976
Batch loss: 0.552855908870697
Batch loss: 0.5530628561973572
Batch loss: 0.5520681142807007
Batch loss: 0.5524818301200867
Batch loss: 0.5526618957519531
Batch loss: 0.5526147484779358
Batch loss: 0.5519043803215027
Epoch 75/100
Validation...
Validation loss: 0.5526, Accuracy: 1.0000
Epoch 75 completed.

Training...
Batch loss: 0.5518717169761658
Batch loss: 0.5524158477783203
Batch loss: 0.5520932078361511
Batch loss: 0.5532029867172241
Batch loss: 0.5522365570068359
Batch loss: 0.5527823567390442
Batch loss: 0.552788496017456
Batch loss: 0.5529575347900391
Batch loss: 0.5527593493461609
Batch loss: 0.5524365901947021
Batch loss: 0.552605390548706
Batch loss: 0.5527557134628296
Batch loss: 0.5533092021942139
Epoch 76/100
Validation...
Validation loss: 0.5526, Accuracy: 1.0000
Epoch 76 completed.

Training...
Batch loss: 0.5525752902030945
Batch loss: 0.552574872970581
Batch loss: 0.552770733833313
Batch loss: 0.5524272918701172
Batch loss: 0.5522350668907166
Batch loss: 0.5523730516433716
Batch loss: 0.5525485277175903
Batch loss: 0.552372395992279
Batch loss: 0.553067684173584
Batch loss: 0.5527245402336121
Batch loss: 0.5523346662521362
Batch loss: 0.5527101755142212
Batch loss: 0.5518769025802612
Epoch 77/100
Validation...
Validation loss: 0.5525, Accuracy: 1.0000
Epoch 77 completed.

Training...
Batch loss: 0.5529152750968933
Batch loss: 0.5525504350662231
Batch loss: 0.552858293056488
Batch loss: 0.5525198578834534
Batch loss: 0.5525412559509277
Batch loss: 0.5521721243858337
Batch loss: 0.5523396730422974
Batch loss: 0.5525073409080505
Batch loss: 0.5523436069488525
Batch loss: 0.5523316860198975
Batch loss: 0.5524700284004211
Batch loss: 0.5526399612426758
Batch loss: 0.5518699884414673
Epoch 78/100
Validation...
Validation loss: 0.5525, Accuracy: 1.0000
Epoch 78 completed.

Training...
Batch loss: 0.5523386001586914
Batch loss: 0.5525098443031311
Batch loss: 0.5528109669685364
Batch loss: 0.5526422262191772
Batch loss: 0.5524697303771973
Batch loss: 0.5523163080215454
Batch loss: 0.552315890789032
Batch loss: 0.5522918701171875
Batch loss: 0.5526479482650757
Batch loss: 0.5522913336753845
Batch loss: 0.5524182915687561
Batch loss: 0.5523282885551453
Batch loss: 0.55311518907547
Epoch 79/100
Validation...
Validation loss: 0.5524, Accuracy: 1.0000
Epoch 79 completed.

Training...
Batch loss: 0.5524265766143799
Batch loss: 0.5526340007781982
Batch loss: 0.5521177649497986
Batch loss: 0.5527673363685608
Batch loss: 0.5524163842201233
Batch loss: 0.5527773499488831
Batch loss: 0.5524541735649109
Batch loss: 0.5523070096969604
Batch loss: 0.5522819757461548
Batch loss: 0.5523914694786072
Batch loss: 0.552308201789856
Batch loss: 0.5523818135261536
Batch loss: 0.5517586469650269
Epoch 80/100
Validation...
Validation loss: 0.5524, Accuracy: 1.0000
Epoch 80 completed.

Training...
Batch loss: 0.5523918867111206
Batch loss: 0.5527446269989014
Batch loss: 0.5528983473777771
Batch loss: 0.552727222442627
Batch loss: 0.5523814558982849
Batch loss: 0.551923394203186
Batch loss: 0.5523760318756104
Batch loss: 0.5522188544273376
Batch loss: 0.5522215366363525
Batch loss: 0.5522717237472534
Batch loss: 0.5520997047424316
Batch loss: 0.5522634387016296
Batch loss: 0.553010880947113
Epoch 81/100
Validation...
Validation loss: 0.5524, Accuracy: 1.0000
Epoch 81 completed.

Training...
Batch loss: 0.5522335767745972
Batch loss: 0.5525052547454834
Batch loss: 0.5520240664482117
Batch loss: 0.5525368452072144
Batch loss: 0.5526517629623413
Batch loss: 0.5522355437278748
Batch loss: 0.552503764629364
Batch loss: 0.5520846247673035
Batch loss: 0.5525189638137817
Batch loss: 0.5519551038742065
Batch loss: 0.5526739358901978
Batch loss: 0.5523485541343689
Batch loss: 0.552306056022644
Epoch 82/100
Validation...
Validation loss: 0.5523, Accuracy: 1.0000
Epoch 82 completed.

Training...
Batch loss: 0.5522050857543945
Batch loss: 0.5524744987487793
Batch loss: 0.551876962184906
Batch loss: 0.5523123741149902
Batch loss: 0.5523364543914795
Batch loss: 0.5520755052566528
Batch loss: 0.5521494150161743
Batch loss: 0.5522146224975586
Batch loss: 0.5529236197471619
Batch loss: 0.5524778366088867
Batch loss: 0.5523638129234314
Batch loss: 0.5523213148117065
Batch loss: 0.5529171824455261
Epoch 83/100
Validation...
Validation loss: 0.5523, Accuracy: 1.0000
Epoch 83 completed.

Training...
Batch loss: 0.5526146292686462
Batch loss: 0.5522981882095337
Batch loss: 0.5517670512199402
Batch loss: 0.5522967576980591
Batch loss: 0.5525737404823303
Batch loss: 0.5526146292686462
Batch loss: 0.5523141026496887
Batch loss: 0.5524372458457947
Batch loss: 0.5524609088897705
Batch loss: 0.5522841215133667
Batch loss: 0.5519701838493347
Batch loss: 0.551748514175415
Batch loss: 0.5528551936149597
Epoch 84/100
Validation...
Validation loss: 0.5523, Accuracy: 1.0000
Epoch 84 completed.

Training...
Batch loss: 0.5524277091026306
Batch loss: 0.5522305369377136
Batch loss: 0.552179217338562
Batch loss: 0.5524144172668457
Batch loss: 0.5525782108306885
Batch loss: 0.5520403385162354
Batch loss: 0.5522435903549194
Batch loss: 0.5520520210266113
Batch loss: 0.552238941192627
Batch loss: 0.5524007678031921
Batch loss: 0.5522250533103943
Batch loss: 0.5521368384361267
Batch loss: 0.5522245168685913
Epoch 85/100
Validation...
Validation loss: 0.5522, Accuracy: 1.0000
Epoch 85 completed.

Training...
Batch loss: 0.5521329641342163
Batch loss: 0.552656888961792
Batch loss: 0.5519843101501465
Batch loss: 0.5520693063735962
Batch loss: 0.5522904396057129
Batch loss: 0.5522147417068481
Batch loss: 0.5522443056106567
Batch loss: 0.5522516965866089
Batch loss: 0.5521188974380493
Batch loss: 0.5521031618118286
Batch loss: 0.551849365234375
Batch loss: 0.5527741312980652
Batch loss: 0.5527662634849548
Epoch 86/100
Validation...
Validation loss: 0.5522, Accuracy: 1.0000
Epoch 86 completed.

Training...
Batch loss: 0.5524824857711792
Batch loss: 0.5523555278778076
Batch loss: 0.5522459745407104
Batch loss: 0.552121102809906
Batch loss: 0.5523639917373657
Batch loss: 0.5518618822097778
Batch loss: 0.5524517297744751
Batch loss: 0.5519441962242126
Batch loss: 0.5519747734069824
Batch loss: 0.5525979399681091
Batch loss: 0.5523273348808289
Batch loss: 0.5517998337745667
Batch loss: 0.552186131477356
Epoch 87/100
Validation...
Validation loss: 0.5522, Accuracy: 1.0000
Epoch 87 completed.

Training...
Batch loss: 0.5518370866775513
Batch loss: 0.5522029995918274
Batch loss: 0.5521052479743958
Batch loss: 0.5521727800369263
Batch loss: 0.5521818399429321
Batch loss: 0.5523209571838379
Batch loss: 0.5521761775016785
Batch loss: 0.5520691275596619
Batch loss: 0.5522827506065369
Batch loss: 0.5518316030502319
Batch loss: 0.5524429678916931
Batch loss: 0.5524670481681824
Batch loss: 0.5526917576789856
Epoch 88/100
Validation...
Validation loss: 0.5522, Accuracy: 1.0000
Epoch 88 completed.

Training...
Batch loss: 0.5524169206619263
Batch loss: 0.5521565079689026
Batch loss: 0.5522563457489014
Batch loss: 0.5521655082702637
Batch loss: 0.5520557761192322
Batch loss: 0.5522934198379517
Batch loss: 0.5518215894699097
Batch loss: 0.5520560145378113
Batch loss: 0.5522162318229675
Batch loss: 0.5520395636558533
Batch loss: 0.5525267720222473
Batch loss: 0.5520424842834473
Batch loss: 0.5517603754997253
Epoch 89/100
Validation...
Validation loss: 0.5521, Accuracy: 1.0000
Epoch 89 completed.

Training...
Batch loss: 0.5520612001419067
Batch loss: 0.5526509284973145
Batch loss: 0.5521410703659058
Batch loss: 0.5521624088287354
Batch loss: 0.5518922805786133
Batch loss: 0.5522821545600891
Batch loss: 0.5523576736450195
Batch loss: 0.5518358945846558
Batch loss: 0.5522444844245911
Batch loss: 0.5519098043441772
Batch loss: 0.5519852042198181
Batch loss: 0.5520365238189697
Batch loss: 0.5526215434074402
Epoch 90/100
Validation...
Validation loss: 0.5521, Accuracy: 1.0000
Epoch 90 completed.

Training...
Batch loss: 0.5521541237831116
Batch loss: 0.5518370866775513
Batch loss: 0.5520042181015015
Batch loss: 0.5522006154060364
Batch loss: 0.5520926117897034
Batch loss: 0.5522593855857849
Batch loss: 0.5523337125778198
Batch loss: 0.5521737933158875
Batch loss: 0.5521186590194702
Batch loss: 0.5519986152648926
Batch loss: 0.552111029624939
Batch loss: 0.5520248413085938
Batch loss: 0.5525880455970764
Epoch 91/100
Validation...
Validation loss: 0.5521, Accuracy: 1.0000
Epoch 91 completed.

Training...
Batch loss: 0.5518696308135986
Batch loss: 0.5520280599594116
Batch loss: 0.5518617033958435
Batch loss: 0.5522443056106567
Batch loss: 0.5523675084114075
Batch loss: 0.5522996187210083
Batch loss: 0.5522146821022034
Batch loss: 0.5522163510322571
Batch loss: 0.552154004573822
Batch loss: 0.5520750284194946
Batch loss: 0.5521785020828247
Batch loss: 0.5517702102661133
Batch loss: 0.551741898059845
Epoch 92/100
Validation...
Validation loss: 0.5521, Accuracy: 1.0000
Epoch 92 completed.

Training...
Batch loss: 0.5521694421768188
Batch loss: 0.5520723462104797
Batch loss: 0.5519229173660278
Batch loss: 0.5519787073135376
Batch loss: 0.5520711541175842
Batch loss: 0.5520660281181335
Batch loss: 0.5519922971725464
Batch loss: 0.552415132522583
Batch loss: 0.5523156523704529
Batch loss: 0.5519862174987793
Batch loss: 0.5520044565200806
Batch loss: 0.5519847273826599
Batch loss: 0.5520204305648804
Epoch 93/100
Validation...
Validation loss: 0.5521, Accuracy: 1.0000
Epoch 93 completed.

Training...
Batch loss: 0.5517771244049072
Batch loss: 0.5523832440376282
Batch loss: 0.5518823862075806
Batch loss: 0.5520535707473755
Batch loss: 0.5519636869430542
Batch loss: 0.5520426034927368
Batch loss: 0.551848828792572
Batch loss: 0.5521891117095947
Batch loss: 0.5522585511207581
Batch loss: 0.5520452260971069
Batch loss: 0.5521972179412842
Batch loss: 0.5521900653839111
Batch loss: 0.5517257452011108
Epoch 94/100
Validation...
Validation loss: 0.5520, Accuracy: 1.0000
Epoch 94 completed.

Training...
Batch loss: 0.5519791841506958
Batch loss: 0.552264928817749
Batch loss: 0.5522814989089966
Batch loss: 0.5522320866584778
Batch loss: 0.5518240928649902
Batch loss: 0.5522965788841248
Batch loss: 0.5518196225166321
Batch loss: 0.5521028637886047
Batch loss: 0.5520094633102417
Batch loss: 0.5519728064537048
Batch loss: 0.5520204901695251
Batch loss: 0.5518472790718079
Batch loss: 0.5516483187675476
Epoch 95/100
Validation...
Validation loss: 0.5520, Accuracy: 1.0000
Epoch 95 completed.

Training...
Batch loss: 0.5519551634788513
Batch loss: 0.551873505115509
Batch loss: 0.5520464181900024
Batch loss: 0.5521599650382996
Batch loss: 0.5522783994674683
Batch loss: 0.551858127117157
Batch loss: 0.5520132184028625
Batch loss: 0.5520281195640564
Batch loss: 0.5519973039627075
Batch loss: 0.5521248579025269
Batch loss: 0.5521511435508728
Batch loss: 0.5519636869430542
Batch loss: 0.5516455769538879
Epoch 96/100
Validation...
Validation loss: 0.5520, Accuracy: 1.0000
Epoch 96 completed.

Training...
Batch loss: 0.5522054433822632
Batch loss: 0.5519435405731201
Batch loss: 0.5517764687538147
Batch loss: 0.5519423484802246
Batch loss: 0.5521262884140015
Batch loss: 0.5518078207969666
Batch loss: 0.5521235466003418
Batch loss: 0.5523184537887573
Batch loss: 0.5518945455551147
Batch loss: 0.5519471168518066
Batch loss: 0.5520685911178589
Batch loss: 0.5519151091575623
Batch loss: 0.552430272102356
Epoch 97/100
Validation...
Validation loss: 0.5520, Accuracy: 1.0000
Epoch 97 completed.

Training...
Batch loss: 0.5523132085800171
Batch loss: 0.5519906878471375
Batch loss: 0.5518489480018616
Batch loss: 0.5519372820854187
Batch loss: 0.5519757270812988
Batch loss: 0.5519877076148987
Batch loss: 0.551916241645813
Batch loss: 0.552040696144104
Batch loss: 0.5521811842918396
Batch loss: 0.5521844029426575
Batch loss: 0.5519142746925354
Batch loss: 0.5517805218696594
Batch loss: 0.5516942143440247
Epoch 98/100
Validation...
Validation loss: 0.5520, Accuracy: 1.0000
Epoch 98 completed.

Training...
Batch loss: 0.5520919561386108
Batch loss: 0.5519241094589233
Batch loss: 0.5519874095916748
Batch loss: 0.5517289042472839
Batch loss: 0.5519711375236511
Batch loss: 0.5517931580543518
Batch loss: 0.5519683957099915
Batch loss: 0.5519286394119263
Batch loss: 0.5517622828483582
Batch loss: 0.5521538257598877
Batch loss: 0.5523897409439087
Batch loss: 0.5520187616348267
Batch loss: 0.5523834824562073
Epoch 99/100
Validation...
Validation loss: 0.5520, Accuracy: 1.0000
Epoch 99 completed.

Training...
Batch loss: 0.5518985986709595
Batch loss: 0.5520871877670288
Batch loss: 0.5519742965698242
Batch loss: 0.5520674586296082
Batch loss: 0.5518641471862793
Batch loss: 0.5520147681236267
Batch loss: 0.5518357753753662
Batch loss: 0.5520158410072327
Batch loss: 0.5517878532409668
Batch loss: 0.5520696640014648
Batch loss: 0.5519909858703613
Batch loss: 0.5520652532577515
Batch loss: 0.5519018173217773
Epoch 100/100
Validation...
Validation loss: 0.5520, Accuracy: 1.0000
Epoch 100 completed.

Training...
Batch loss: 0.5521894693374634
Batch loss: 0.5517868399620056
Batch loss: 0.551872730255127
Batch loss: 0.5520691275596619
Batch loss: 0.5519038438796997
Batch loss: 0.5522359013557434
Batch loss: 0.552036464214325
Batch loss: 0.5517638921737671
Batch loss: 0.5520477294921875
Batch loss: 0.551940381526947
Batch loss: 0.5518607497215271
Batch loss: 0.5517985820770264
Batch loss: 0.5519012808799744
